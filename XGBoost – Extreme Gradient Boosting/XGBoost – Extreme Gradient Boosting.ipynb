{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost – Extreme Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **O que é XGBoost – Extreme Gradient Boosting?**\n",
    "\n",
    "O desempenho do XGBoost foi amplamente reconhecido em várias competições de machine learning e extração de dados. Por exemplo, nas competições hospedadas no site Kaggle de aprendizado de máquina. Das 29 soluções vencedoras publicadas no Kaggle durante 2015, 17 utilizaram o XGBoost. Para comparação, o segundo método mais popular, redes neurais profundas, foi usado em 11 soluções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/enzoschitini/machine-learning/refs/heads/Ensemble-Techniques/img/Timeline%20of%20algorithm%20evolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O sucesso do XGBoost também foi evidenciado na KDDCup 2015, onde o XGBoost foi utilizado por todas as equipes vencedoras dos primeiros 10 lugares. O fator mais importante por trás do sucesso do XGBoost é sua escalabilidade em todos os cenários. O XGBoost funciona mais de dez vezes mais rápido do que as soluções mais comuns em uma única máquina e pode ser estendido para bilhões de exemplos em configurações distribuídas ou com restrições de memória. \n",
    "\n",
    "A escalabilidade do XGBoost é atribuída a vários sistemas e otimizações algorítmicas importantes. Essas inovações incluem: um novo algoritmo de aprendizado de árvores para lidar com dados esparsos, quartis ponderados, etc. O cálculo paralelo e distribuído torna o aprendizado mais rápido, permitindo uma exploração mais rápida do modelo.\n",
    "\n",
    "*Chen e Guestrin, 2016*\n",
    "\n",
    "**XGBoost (Extreme Gradient Boosting)** é um algoritmo de machine learning baseado na técnica de boosting, utilizado principalmente para problemas de classificação e regressão. É conhecido por sua eficiência, flexibilidade e precisão. Abaixo está uma explicação detalhada dos principais aspectos do XGBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/enzoschitini/machine-learning/refs/heads/Ensemble-Techniques/img/XGB.png)\n",
    "Fonte: https://www.ictrading.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Gradient Boosting**\n",
    "\n",
    "O XGBoost se baseia em um método chamado gradient boosting, que constrói modelos de previsão na forma de um ensemble de modelos fracos, tipicamente árvores de decisão. A ideia é combinar vários modelos fracos para formar um modelo forte. Cada novo modelo tenta corrigir os erros cometidos pelos modelos anteriores.\n",
    "\n",
    "### 2. **Iterative Learning**\n",
    "\n",
    "O processo de aprendizado no XGBoost é iterativo:\n",
    "\n",
    "- **Initialization**: Começa-se com uma estimativa inicial, geralmente a média (para problemas de regressão) ou a moda (para problemas de classificação) dos alvos.\n",
    "- **Additive Model**: A cada iteração, uma nova árvore de decisão é adicionada para tentar corrigir os erros residuais (residuals) do modelo anterior.\n",
    "- **Optimization**: Isso é obtido minimizando uma função de perda (por exemplo, erro quadrático médio para regressão ou entropia para classificação).\n",
    "\n",
    "### 3. **Extreme Gradient Boosting**\n",
    "\n",
    "A extensão do XGBoost ao tradicional gradient boosting envolve várias melhorias:\n",
    "\n",
    "- **Regularization**: O XGBoost inclui termos de regularização para evitar o overfitting, tornando o modelo mais robusto.\n",
    "- **Sparsity Aware**: Lida eficientemente com dados esparsos (com muitos valores nulos ou zero) por meio de uma técnica chamada \"Sparsity-aware Split Finding\".\n",
    "- **Parallel Computing**: O XGBoost é projetado para ser escalável e pode aproveitar o paralelismo para acelerar o processo de aprendizado.\n",
    "- **Pruning**: Utiliza uma técnica chamada \"max depth\" para evitar o crescimento excessivo das árvores, mantendo o modelo simples e rápido.\n",
    "- **Weighted Quantile Sketch**: Essa técnica permite gerenciar pesos não uniformes nos dados, melhorando a flexibilidade do algoritmo.\n",
    "- **Cross-Validation**: Suporta validação cruzada interna durante o treinamento para melhorar a confiabilidade do modelo.\n",
    "\n",
    "### 4. **Importância das Features**\n",
    "\n",
    "O XGBoost fornece ferramentas para avaliar a importância das variáveis preditivas, ajudando na seleção de features e na compreensão do modelo.\n",
    "\n",
    "### 5. **Aplicações**\n",
    "\n",
    "O XGBoost é amplamente utilizado em competições de machine learning (como no Kaggle) e em vários setores, como finanças, marketing e bioinformática, devido à sua capacidade de lidar com grandes quantidades de dados e produzir modelos precisos.\n",
    "\n",
    "### 6. **Vantagens e Desvantagens**\n",
    "\n",
    "**Vantagens:**\n",
    "\n",
    "- Alta precisão\n",
    "- Boa gestão dos dados faltantes\n",
    "- Escalabilidade\n",
    "- Flexibilidade nos parâmetros de configuração\n",
    "\n",
    "**Desvantagens:**\n",
    "\n",
    "- Necessita de uma boa compreensão dos parâmetros para otimização\n",
    "- Maior complexidade em comparação com métodos mais simples, como as árvores de decisão individuais\n",
    "\n",
    "O XGBoost é, portanto, uma ferramenta poderosa para modelagem preditiva, mas requer uma certa experiência e experimentação para obter os melhores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferenças entre XGBoost – Extreme Gradient Boosting e Gradient Boosting Machine:\n",
    "\n",
    "O XGBoost (Extreme Gradient Boosting) e o Gradient Boosting Machine (GBM) são ambos algoritmos de boosting usados para melhorar o desempenho preditivo, combinando vários modelos fracos. No entanto, existem algumas diferenças chave entre os dois.\n",
    "\n",
    "Aqui está uma tabela que resume as principais diferenças entre XGBoost e Gradient Boosting Machine (GBM):\n",
    "\n",
    "| **Característica** | **XGBoost (Extreme Gradient Boosting)** | **GBM (Gradient Boosting Machine)** |\n",
    "| --- | --- | --- |\n",
    "| **Regularização** | Inclui regularização L1 e L2 | Não inclui regularização por padrão |\n",
    "| **Gestão de Dados Esparsos** | Otimizado para dados esparsos | Não otimizado para dados esparsos |\n",
    "| **Parallel Computing** | Suporta paralelismo | Não suporta paralelismo por padrão |\n",
    "| **Pruning** | Utiliza pruning baseado em max_depth | Não inclui pruning por padrão |\n",
    "| **Gestão de Valores Faltantes** | Gerencia automaticamente valores faltantes | Não gerencia automaticamente valores faltantes |\n",
    "| **Weighted Quantile Sketch** | Suporta pesos não uniformes | Não suporta |\n",
    "| **Desempenho** | Geralmente mais rápido | Mais lento que o XGBoost |\n",
    "| **Escalabilidade** | Altamente escalável em grandes datasets | Menos escalável que o XGBoost |\n",
    "| **Cross-Validation** | Suporta cross-validation interna | Cross-validation gerida separadamente |\n",
    "| **Flexibilidade** | Amplo conjunto de parâmetros para ajuste | Conjunto de parâmetros menos detalhado |\n",
    "| **Comunidade e Suporte** | Comunidade muito ativa e amplo suporte | Comunidade ativa, mas menos extensa que a do XGBoost |\n",
    "| **Adoção em Competições** | Ampliamente utilizado em competições (ex. Kaggle) | Menos popular em competições comparado ao XGBoost |\n",
    "| **Uso em Aplicações** | Usado em vários setores (finanças, marketing, bioinformática) | Usado em aplicações comerciais e acadêmicas |\n",
    "\n",
    "Esta tabela destaca como o XGBoost inclui várias otimizações e funcionalidades adicionais que o tornam mais eficiente e versátil em comparação ao GBM tradicional. No entanto, a escolha entre os dois depende das necessidades específicas do projeto e dos recursos disponíveis.\n",
    "\n",
    "Em resumo, o XGBoost é uma implementação otimizada e avançada do gradient boosting que inclui várias características adicionais, como regularização, suporte para dados esparsos e otimização do paralelismo. O GBM é a versão tradicional do gradient boosting, menos otimizada, mas ainda poderosa e amplamente utilizada. A escolha entre os dois depende das necessidades específicas do projeto, do tamanho do dataset e das capacidades computacionais disponíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparâmetros importantes no XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "Os hiperparâmetros no XGBoost desempenham um papel crucial na determinação do desempenho do modelo. Aqui está uma lista dos principais hiperparâmetros no XGBoost, divididos por categoria, com uma breve descrição de cada um:\n",
    "\n",
    "### 1. **Hiperparâmetros Gerais**\n",
    "\n",
    "- **booster**: Especifica o tipo de booster a ser utilizado (por exemplo, 'gbtree', 'gblinear', 'dart'). O valor padrão é 'gbtree'.\n",
    "\n",
    "### 2. **Hiperparâmetros das Árvores (Tree Booster)**\n",
    "\n",
    "- **eta (learning_rate)**: Taxa de aprendizado. Valores mais baixos tornam o modelo mais robusto, mas exigem mais iterações (padrão: 0.3).\n",
    "- **max_depth**: Profundidade máxima das árvores. Valores mais altos podem aumentar a complexidade do modelo e o risco de overfitting (padrão: 6).\n",
    "- **min_child_weight**: Peso mínimo da soma para um nó folha. Valores mais altos tornam o modelo mais conservador (padrão: 1).\n",
    "- **gamma**: Redução mínima da perda necessária para fazer uma divisão adicional. Valores mais altos tornam o modelo mais conservador (padrão: 0).\n",
    "- **subsample**: Fração de amostras usadas para crescer cada árvore. Valores menores que 1.0 podem prevenir overfitting (padrão: 1).\n",
    "- **colsample_bytree**: Fração de características usadas para crescer cada árvore. Valores menores que 1.0 podem prevenir overfitting (padrão: 1).\n",
    "- **colsample_bylevel**: Fração de características usadas para cada nível de uma árvore (padrão: 1).\n",
    "- **colsample_bynode**: Fração de características usadas para cada nó (padrão: 1).\n",
    "\n",
    "### 3. **Hiperparâmetros de Regularização**\n",
    "\n",
    "- **lambda (reg_lambda)**: Termos de regularização L2 (Ridge). Ajuda a prevenir o overfitting (padrão: 1).\n",
    "- **alpha (reg_alpha)**: Termos de regularização L1 (Lasso). Ajuda a prevenir o overfitting (padrão: 0).\n",
    "\n",
    "### 4. **Hiperparâmetros de Dart (Dropouts meet Multiple Additive Regression Trees)**\n",
    "\n",
    "- **sample_type**: Tipo de amostragem para dropout ('uniform', 'weighted'). Padrão: 'uniform'.\n",
    "- **normalize_type**: Método de normalização para dropout ('tree', 'forest'). Padrão: 'tree'.\n",
    "- **rate_drop**: Probabilidade de dropout para uma árvore durante o boosting. Padrão: 0.0.\n",
    "- **skip_drop**: Probabilidade de pular o dropout durante o boosting. Padrão: 0.0.\n",
    "\n",
    "### 5. **Hiperparâmetros do Linear Booster**\n",
    "\n",
    "- **lambda (reg_lambda)**: Termos de regularização L2. Padrão: 0.\n",
    "- **alpha (reg_alpha)**: Termos de regularização L1. Padrão: 0.\n",
    "- **lambda_bias**: Termo de regularização para o bias. Padrão: 0.\n",
    "\n",
    "### 6. **Hiperparâmetros de Otimização**\n",
    "\n",
    "- **objective**: Função de perda a ser minimizada (por exemplo, 'reg', 'binary'). Deve ser configurado conforme o problema.\n",
    "- **eval_metric**: Métrica de avaliação para validação (por exemplo, 'rmse', 'logloss', 'error'). Pode ser utilizada para monitorar o desempenho do modelo durante o treinamento.\n",
    "- **n_estimators**: Número de árvores (iterações de boosting). Valores mais altos podem melhorar o desempenho, mas aumentam o risco de overfitting (padrão: 100).\n",
    "- **seed**: Número da semente para a reprodutibilidade dos resultados. Padrão: None.\n",
    "\n",
    "### 7. **Hiperparâmetros de Early Stopping**\n",
    "\n",
    "- **early_stopping_rounds**: Número de iterações sem melhorias para interromper o treinamento antecipadamente. Usado com o conjunto de validação."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
