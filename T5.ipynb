{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Tópicos Avançados**\n",
    "\n",
    "Nesta seção, exploramos alguns conceitos avançados do PCA, abordando desde a interpretação dos componentes até as limitações do PCA e alternativas para casos específicos.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.1 Interpretação dos Componentes**\n",
    "\n",
    "### **Como entender o significado dos componentes principais**\n",
    "\n",
    "Os **componentes principais** do PCA são combinações lineares das variáveis originais que capturam as direções de maior variância nos dados. A interpretação de cada componente depende de como ele é formado a partir das variáveis originais. Por exemplo:\n",
    "\n",
    "- Se um componente principal é uma combinação forte de variáveis relacionadas a altura e peso, podemos interpretá-lo como uma \"medida de tamanho corporal\".\n",
    "- O **peso das variáveis** em cada componente pode ser obtido observando os **autovetores**. Cada autovetor fornece o peso relativo de cada variável na direção do componente principal. Se um autovetor tem valores altos para algumas variáveis, isso significa que essas variáveis têm uma grande contribuição para o componente principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset Wine\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "X, feature_names = wine.data, wine.feature_names\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# Pesos das variáveis nos componentes principais\n",
    "components = pca.components_\n",
    "\n",
    "# Exibir os pesos\n",
    "for i, component in enumerate(components):\n",
    "    print(f\"Component {i+1}:\")\n",
    "    for weight, feature in sorted(zip(component, feature_names), key=lambda x: -abs(x[0])):\n",
    "        print(f\"  {feature}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Como relacioná-los com as variáveis originais**\n",
    "\n",
    "- Cada componente principal pode ser visto como uma \"nova variável\" que é uma combinação ponderada das variáveis originais. Os **autovalores** indicam a importância de cada componente, e a **carga de cada variável** no autovetor indica a relevância dessa variável para o componente.\n",
    "- A interpretação pode ser facilitada examinando os **autovetores**, que indicam como as variáveis originais são projetadas no novo espaço. Uma variável que contribui muito para um componente terá um coeficiente alto no autovetor correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plotar a contribuição das variáveis no PC1 e PC2\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(feature_names, components[0], label=\"PC1\", alpha=0.7)\n",
    "plt.bar(feature_names, components[1], label=\"PC2\", alpha=0.7)\n",
    "plt.title(\"Contribuição das Variáveis nos Componentes Principais\")\n",
    "plt.ylabel(\"Peso\")\n",
    "plt.xlabel(\"Variáveis Originais\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **5.2 Escolha do Número de Componentes**\n",
    "\n",
    "A escolha do número de componentes a ser retido é uma das questões cruciais no PCA.\n",
    "\n",
    "### **Critérios para selecionar o número ideal**\n",
    "\n",
    "1. **Variância explicada cumulativa:**\n",
    "    - O número ideal de componentes é muitas vezes escolhido com base na **variância explicada cumulativa**. Por exemplo, pode-se optar por um número de componentes que explique 95% da variância total dos dados.\n",
    "    - A fórmula é simples: a variância explicada acumulada é a soma dos autovalores até o componente  dividido pela soma total dos autovalores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variância explicada cumulativa\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "plt.axhline(0.9, color='r', linestyle='--', label='90% Variância Explicada')\n",
    "plt.title(\"Variância Explicada Cumulativa\")\n",
    "plt.xlabel(\"Número de Componentes\")\n",
    "plt.ylabel(\"Proporção Cumulativa\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Critério de Kaiser:**\n",
    "    - O critério de Kaiser sugere que se deve manter apenas os componentes com **autovalores maiores que 1**. Isso ocorre porque um componente com autovalor menor que 1 não explica mais do que uma variável original, e, portanto, não tem uma contribuição significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critério de Kaiser\n",
    "eigenvalues = pca.explained_variance_\n",
    "kaiser_components = sum(eigenvalues > 1)\n",
    "print(f\"Componentes selecionados pelo Critério de Kaiser: {kaiser_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Análise do scree plot:**\n",
    "    - O **scree plot** é outro método comum para selecionar o número de componentes. O gráfico mostra a variância explicada por cada componente. A partir do ponto de inflexão (onde a variância explicada por novos componentes começa a diminuir drasticamente), é possível escolher quantos componentes manter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='--', color='g')\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Número de Componentes\")\n",
    "plt.ylabel(\"Autovalor\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **5.3 PCA para Dados Não Lineares**\n",
    "\n",
    "### **Limitações do PCA para capturar relações não lineares**\n",
    "\n",
    "- O PCA é eficaz apenas para capturar **relações lineares** entre as variáveis. Se os dados tiverem **estruturas não lineares** (por exemplo, relações curvilíneas entre as variáveis), o PCA pode não ser capaz de representar completamente essas relações.\n",
    "- Além disso, o PCA pode falhar em preservar a estrutura dos dados em casos em que as variações mais importantes estão em uma direção não linear.\n",
    "\n",
    "### **Métodos alternativos: Kernel PCA**\n",
    "\n",
    "- O **Kernel PCA** é uma extensão do PCA tradicional que usa o conceito de **mapeamento de alta dimensão** para capturar relações não lineares. Ele utiliza funções de kernel (como o kernel Gaussiano) para transformar os dados em um espaço de maior dimensionalidade, onde as relações lineares podem ser capturadas.\n",
    "- Em vez de calcular a matriz de covariância diretamente, o Kernel PCA aplica uma transformação no espaço de características para encontrar componentes principais no espaço transformado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Aplicar Kernel PCA com núcleo RBF\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# Visualizar os dados após Kernel PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=wine.target, cmap='viridis', s=50)\n",
    "plt.title(\"Projeção com Kernel PCA (RBF)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.4 PCA e Compressão de Dados**\n",
    "\n",
    "### **Uso do PCA para reduzir o espaço de armazenamento**\n",
    "\n",
    "- O PCA é amplamente utilizado para **compressão de dados**. Ao reduzir a dimensionalidade dos dados, o PCA pode diminuir significativamente a quantidade de espaço necessária para armazenar os dados.\n",
    "- Por exemplo, em imagens de alta resolução, o PCA pode ser usado para reduzir a quantidade de pixels enquanto retém as características mais importantes, o que reduz o tamanho do arquivo sem perda significativa de qualidade visual.\n",
    "\n",
    "### **Comparação com outros métodos de compressão**\n",
    "\n",
    "- Comparado a outros métodos de compressão como **compressão por Huffman** ou **transformada wavelet**, o PCA oferece uma compressão mais centrada na **variação dos dados**. Métodos como Huffman são mais adequados para compressão sem perdas, enquanto o PCA visa manter as variâncias mais importantes, sacrificando dados menos relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregar a imagem em tons de cinza\n",
    "image = data.camera()\n",
    "image_scaled = image / 255.0\n",
    "\n",
    "# Aplicar PCA para compressão\n",
    "n_components = 50\n",
    "pca = PCA(n_components=n_components)\n",
    "compressed = pca.fit_transform(image_scaled)\n",
    "reconstructed = pca.inverse_transform(compressed)\n",
    "\n",
    "# Comparação de tamanho\n",
    "original_size = image.size\n",
    "compressed_size = compressed.size + pca.components_.size\n",
    "compression_ratio = (1 - compressed_size / original_size) * 100\n",
    "print(f\"Redução no espaço de armazenamento: {compression_ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.5 PCA em Dados Categóricos**\n",
    "\n",
    "### **Limitações do PCA para variáveis categóricas**\n",
    "\n",
    "- O PCA foi desenvolvido para trabalhar com variáveis **contínuas** e pode não ser eficaz para **dados categóricos**, como variáveis nominais ou ordinais. Como o PCA usa covariância e cálculos lineares, ele não consegue capturar as relações entre categorias distintas de maneira adequada.\n",
    "- Em dados categóricos, o PCA pode gerar resultados imprecisos ou sem interpretação clara, já que ele não leva em consideração a natureza qualitativa das variáveis.\n",
    "\n",
    "### **Métodos alternativos: MCA (Multiple Correspondence Analysis)**\n",
    "\n",
    "- A **Análise de Correspondência Múltipla (MCA)** é uma técnica semelhante ao PCA, mas projetada especificamente para dados categóricos. Ela pode ser vista como uma generalização do PCA para variáveis qualitativas.\n",
    "- MCA encontra padrões e associações entre categorias, permitindo uma redução de dimensionalidade sem perder a interpretação associada às variáveis categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "\n",
    "# Dataset categórico de exemplo\n",
    "data = pd.DataFrame({\n",
    "    'Cor': ['Vermelho', 'Azul', 'Verde', 'Vermelho', 'Azul'],\n",
    "    'Forma': ['Círculo', 'Quadrado', 'Triângulo', 'Círculo', 'Quadrado']\n",
    "})\n",
    "\n",
    "# Aplicar MCA\n",
    "mca = prince.MCA(n_components=2, random_state=42)\n",
    "mca_result = mca.fit_transform(data)\n",
    "\n",
    "# Visualizar os resultados\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(mca_result.iloc[:, 0], mca_result.iloc[:, 1], color='b', s=100)\n",
    "plt.title(\"Projeção com MCA (Dados Categóricos)\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resumo das Adições**\n",
    "\n",
    "- **5.1**: Interpretamos os componentes principais com pesos e gráficos.\n",
    "- **5.2**: Adicionamos critérios para escolher o número de componentes, como scree plot e Kaiser.\n",
    "- **5.3**: Incluímos Kernel PCA para lidar com não linearidades.\n",
    "- **5.4**: Demonstramos compressão de dados e cálculo do ganho de armazenamento.\n",
    "- **5.5**: Apresentamos MCA como alternativa para dados categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pacotes necessários\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from skimage import data\n",
    "\n",
    "# Carregar o dataset Wine\n",
    "wine = load_wine()\n",
    "X, feature_names = wine.data, wine.feature_names\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# Pesos das variáveis nos componentes principais\n",
    "components = pca.components_\n",
    "\n",
    "# Exibir os pesos\n",
    "for i, component in enumerate(components):\n",
    "    print(f\"Component {i+1}:\")\n",
    "    for weight, feature in sorted(zip(component, feature_names), key=lambda x: -abs(x[0])):\n",
    "        print(f\"  {feature}: {weight:.2f}\")\n",
    "\n",
    "# Plotar a contribuição das variáveis no PC1 e PC2\n",
    "plt.figure(figsize=(8, 6))\n",
    "x = np.arange(len(feature_names))\n",
    "plt.bar(x - 0.2, components[0], width=0.4, label=\"PC1\", alpha=0.7)\n",
    "plt.bar(x + 0.2, components[1], width=0.4, label=\"PC2\", alpha=0.7)\n",
    "plt.title(\"Contribuição das Variáveis nos Componentes Principais\")\n",
    "plt.ylabel(\"Peso\")\n",
    "plt.xlabel(\"Variáveis Originais\")\n",
    "plt.xticks(x, feature_names, rotation=90)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Variância explicada cumulativa\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "plt.axhline(0.9, color='r', linestyle='--', label='90% Variância Explicada')\n",
    "plt.title(\"Variância Explicada Cumulativa\")\n",
    "plt.xlabel(\"Número de Componentes\")\n",
    "plt.ylabel(\"Proporção Cumulativa\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Critério de Kaiser\n",
    "eigenvalues = pca.explained_variance_\n",
    "kaiser_components = sum(eigenvalues > 1)\n",
    "print(f\"Componentes selecionados pelo Critério de Kaiser: {kaiser_components}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='--', color='g')\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Número de Componentes\")\n",
    "plt.ylabel(\"Autovalor\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Aplicar Kernel PCA com núcleo RBF\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# Visualizar os dados após Kernel PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=wine.target, cmap='viridis', s=50)\n",
    "plt.title(\"Projeção com Kernel PCA (RBF)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Carregar a imagem em tons de cinza\n",
    "image = data.camera()\n",
    "image_scaled = image / 255.0\n",
    "\n",
    "# Aplicar PCA para compressão\n",
    "n_components = 50\n",
    "pca = PCA(n_components=n_components)\n",
    "compressed = pca.fit_transform(image_scaled)\n",
    "reconstructed = pca.inverse_transform(compressed)\n",
    "\n",
    "# Comparação de tamanho\n",
    "original_size = image.size\n",
    "compressed_size = compressed.size + pca.components_.size\n",
    "compression_ratio = (1 - compressed_size / original_size) * 100\n",
    "print(f\"Redução no espaço de armazenamento: {compression_ratio:.2f}%\")\n",
    "\n",
    "# Dataset categórico de exemplo\n",
    "data_categorical = pd.DataFrame({\n",
    "    'Cor': ['Vermelho', 'Azul', 'Verde', 'Vermelho', 'Azul'],\n",
    "    'Forma': ['Círculo', 'Quadrado', 'Triângulo', 'Círculo', 'Quadrado']\n",
    "})\n",
    "\n",
    "# Aplicar MCA\n",
    "import prince\n",
    "mca = prince.MCA(n_components=2, random_state=42)\n",
    "mca_result = mca.fit_transform(data_categorical)\n",
    "\n",
    "# Visualizar os resultados\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(mca_result.iloc[:, 0], mca_result.iloc[:, 1], color='b', s=100)\n",
    "plt.title(\"Projeção com MCA (Dados Categóricos)\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
