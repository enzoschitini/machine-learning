{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Tópicos Avançados**\n",
    "\n",
    "Nesta seção, exploramos alguns conceitos avançados do PCA, abordando desde a interpretação dos componentes até as limitações do PCA e alternativas para casos específicos.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.1 Interpretação dos Componentes**\n",
    "\n",
    "### **Como entender o significado dos componentes principais**\n",
    "\n",
    "Os **componentes principais** do PCA são combinações lineares das variáveis originais que capturam as direções de maior variância nos dados. A interpretação de cada componente depende de como ele é formado a partir das variáveis originais. Por exemplo:\n",
    "\n",
    "- Se um componente principal é uma combinação forte de variáveis relacionadas a altura e peso, podemos interpretá-lo como uma \"medida de tamanho corporal\".\n",
    "- O **peso das variáveis** em cada componente pode ser obtido observando os **autovetores**. Cada autovetor fornece o peso relativo de cada variável na direção do componente principal. Se um autovetor tem valores altos para algumas variáveis, isso significa que essas variáveis têm uma grande contribuição para o componente principal.\n",
    "\n",
    "### **Como relacioná-los com as variáveis originais**\n",
    "\n",
    "- Cada componente principal pode ser visto como uma \"nova variável\" que é uma combinação ponderada das variáveis originais. Os **autovalores** indicam a importância de cada componente, e a **carga de cada variável** no autovetor indica a relevância dessa variável para o componente.\n",
    "- A interpretação pode ser facilitada examinando os **autovetores**, que indicam como as variáveis originais são projetadas no novo espaço. Uma variável que contribui muito para um componente terá um coeficiente alto no autovetor correspondente.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.2 Escolha do Número de Componentes**\n",
    "\n",
    "A escolha do número de componentes a ser retido é uma das questões cruciais no PCA.\n",
    "\n",
    "### **Critérios para selecionar o número ideal**\n",
    "\n",
    "1. **Variância explicada cumulativa:**\n",
    "    - O número ideal de componentes é muitas vezes escolhido com base na **variância explicada cumulativa**. Por exemplo, pode-se optar por um número de componentes que explique 95% da variância total dos dados.\n",
    "    - A fórmula é simples: a variância explicada acumulada é a soma dos autovalores até o componente  dividido pela soma total dos autovalores.\n",
    "        \n",
    "        kk\n",
    "        \n",
    "2. **Critério de Kaiser:**\n",
    "    - O critério de Kaiser sugere que se deve manter apenas os componentes com **autovalores maiores que 1**. Isso ocorre porque um componente com autovalor menor que 1 não explica mais do que uma variável original, e, portanto, não tem uma contribuição significativa.\n",
    "3. **Análise do scree plot:**\n",
    "    - O **scree plot** é outro método comum para selecionar o número de componentes. O gráfico mostra a variância explicada por cada componente. A partir do ponto de inflexão (onde a variância explicada por novos componentes começa a diminuir drasticamente), é possível escolher quantos componentes manter.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.3 PCA para Dados Não Lineares**\n",
    "\n",
    "### **Limitações do PCA para capturar relações não lineares**\n",
    "\n",
    "- O PCA é eficaz apenas para capturar **relações lineares** entre as variáveis. Se os dados tiverem **estruturas não lineares** (por exemplo, relações curvilíneas entre as variáveis), o PCA pode não ser capaz de representar completamente essas relações.\n",
    "- Além disso, o PCA pode falhar em preservar a estrutura dos dados em casos em que as variações mais importantes estão em uma direção não linear.\n",
    "\n",
    "### **Métodos alternativos: Kernel PCA**\n",
    "\n",
    "- O **Kernel PCA** é uma extensão do PCA tradicional que usa o conceito de **mapeamento de alta dimensão** para capturar relações não lineares. Ele utiliza funções de kernel (como o kernel Gaussiano) para transformar os dados em um espaço de maior dimensionalidade, onde as relações lineares podem ser capturadas.\n",
    "- Em vez de calcular a matriz de covariância diretamente, o Kernel PCA aplica uma transformação no espaço de características para encontrar componentes principais no espaço transformado.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.4 PCA e Compressão de Dados**\n",
    "\n",
    "### **Uso do PCA para reduzir o espaço de armazenamento**\n",
    "\n",
    "- O PCA é amplamente utilizado para **compressão de dados**. Ao reduzir a dimensionalidade dos dados, o PCA pode diminuir significativamente a quantidade de espaço necessária para armazenar os dados.\n",
    "- Por exemplo, em imagens de alta resolução, o PCA pode ser usado para reduzir a quantidade de pixels enquanto retém as características mais importantes, o que reduz o tamanho do arquivo sem perda significativa de qualidade visual.\n",
    "\n",
    "### **Comparação com outros métodos de compressão**\n",
    "\n",
    "- Comparado a outros métodos de compressão como **compressão por Huffman** ou **transformada wavelet**, o PCA oferece uma compressão mais centrada na **variação dos dados**. Métodos como Huffman são mais adequados para compressão sem perdas, enquanto o PCA visa manter as variâncias mais importantes, sacrificando dados menos relevantes.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.5 PCA em Dados Categóricos**\n",
    "\n",
    "### **Limitações do PCA para variáveis categóricas**\n",
    "\n",
    "- O PCA foi desenvolvido para trabalhar com variáveis **contínuas** e pode não ser eficaz para **dados categóricos**, como variáveis nominais ou ordinais. Como o PCA usa covariância e cálculos lineares, ele não consegue capturar as relações entre categorias distintas de maneira adequada.\n",
    "- Em dados categóricos, o PCA pode gerar resultados imprecisos ou sem interpretação clara, já que ele não leva em consideração a natureza qualitativa das variáveis.\n",
    "\n",
    "### **Métodos alternativos: MCA (Multiple Correspondence Analysis)**\n",
    "\n",
    "- A **Análise de Correspondência Múltipla (MCA)** é uma técnica semelhante ao PCA, mas projetada especificamente para dados categóricos. Ela pode ser vista como uma generalização do PCA para variáveis qualitativas.\n",
    "- MCA encontra padrões e associações entre categorias, permitindo uma redução de dimensionalidade sem perder a interpretação associada às variáveis categóricas.\n",
    "\n",
    "---\n",
    "\n",
    "Esses tópicos avançados são importantes para entender as limitações e alternativas ao PCA, além de permitir uma análise mais detalhada e precisa dos dados. Se você precisar de mais detalhes sobre algum desses pontos ou quiser exemplos específicos, posso ajudá-lo!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
