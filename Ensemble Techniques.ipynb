{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques\n",
    "Os algoritmos de ensemble são técnicas de machine learning que combinam vários modelos base para melhorar o desempenho geral do sistema. A ideia é que, ao agregar as previsões de diferentes modelos, obtém-se uma estimativa mais robusta e precisa em comparação com aquela fornecida por um único modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diferentes tipos de algoritmos de ensemble, mas os principais são:\n",
    "\n",
    "- **Bagging (Bootstrap Aggregating)**:\n",
    "    - **Princípio**: Constrói vários modelos base em subconjuntos aleatórios dos dados de treinamento. Cada subconjunto é obtido usando o método bootstrap (amostragem com reposição).\n",
    "    - **Modelos Típicos**: Random Forest é um exemplo popular de bagging, utilizando decision trees como modelo base.\n",
    "    - **Vantagem**: Reduz a variância do modelo, melhorando a estabilidade e a precisão sem aumentar significativamente o bias.\n",
    "- **Boosting**:\n",
    "    - **Princípio**: Constrói os modelos base um de cada vez, com cada modelo focando nos erros dos modelos anteriores. Os modelos são treinados em sequência, e cada um melhora o anterior.\n",
    "    - **Modelos Típicos**: AdaBoost, Gradient Boosting, XGBoost, LightGBM e CatBoost.\n",
    "    - **Vantagem**: Melhora a precisão reduzindo tanto o bias quanto a variância. É particularmente eficaz em problemas de classificação e regressão.\n",
    "- **Stacking (Stacked Generalization)**:\n",
    "    - **Princípio**: Combina diferentes modelos base em um nível superior por meio de um meta-modelo. Os modelos base produzem suas previsões, que são usadas como entrada para o meta-modelo.\n",
    "    - **Modelo Típico**: Um exemplo é um classificador linear que usa as saídas de vários modelos base como entrada.\n",
    "    - **Vantagem**: Permite aproveitar os pontos fortes de diferentes modelos para obter previsões mais precisas.\n",
    "- **Blending**:\n",
    "    - **Princípio**: Similar ao stacking, mas utiliza um conjunto de validação separado para treinar o meta-modelo. Não usa o conjunto de teste na criação do modelo.\n",
    "    - **Vantagem**: É mais simples de implementar em comparação com o stacking e reduz o risco de overfitting no conjunto de teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens dos Algoritmos de Ensemble:\n",
    "\n",
    "- **Melhoria da Precisão**: Ao agregar vários modelos, tende-se a reduzir o erro global.\n",
    "- **Robustez**: Os modelos de ensemble são menos suscetíveis ao overfitting em comparação com modelos individuais.\n",
    "- **Flexibilidade**: Podem combinar modelos de diferentes tipos, aproveitando suas características complementares.\n",
    "\n",
    "### Desvantagens:\n",
    "\n",
    "- **Complexidade Computacional**: Os algoritmos de ensemble podem ser mais complexos e demandar mais recursos computacionais.\n",
    "- **Dificuldade na Interpretabilidade**: Como combinam vários modelos, pode ser difícil interpretar o comportamento geral do sistema.\n",
    "\n",
    "Em resumo, os algoritmos de ensemble são uma estratégia poderosa para melhorar o desempenho dos modelos de machine learning, aproveitando a diversidade e a sinergia entre diferentes modelos base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferença entre algoritmos de Bagging e Boosting:\n",
    "\n",
    "Os métodos de Bagging e Boosting são ambos técnicas de ensemble learning utilizadas para melhorar o desempenho dos modelos de machine learning. No entanto, **diferem significativamente na forma como combinam os modelos base** (frequentemente chamados de \"weak learners\" ou \"base learners\") para criar um modelo mais robusto e preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Processo de Treinamento**: O **Bagging** treina modelos base **de forma independente e paralela**, enquanto o **Boosting** treina modelos base **de forma sequencial**, com cada modelo focando nos pontos fracos dos modelos anteriores.\n",
    "- **Atualização de Pesos**: O **Boosting atualiza os pesos dos exemplos durante o treinamento** para se concentrar nos erros, enquanto o **Bagging trata todos os subconjuntos de dados de treinamento de forma equivalente.**\n",
    "- **Combinação de Modelos**: O **Bagging** combina modelos base por meio de uma **agregação simples**, enquanto o **Boosting** combina modelos base **de forma que cada um contribua com um peso variável**, geralmente baseado em seu desempenho.\n",
    "\n",
    "Em resumo, embora ambos os métodos se baseiem na ideia de combinar modelos base para melhorar o desempenho do modelo final, suas abordagens para o amostramento de dados, o treinamento dos modelos e o tratamento de erros os tornam distintos na prática e adequados para diferentes contextos de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é Bagging?\n",
    "\n",
    "O Bagging, abreviação de \"Bootstrap Aggregating\", é uma técnica de ensemble utilizada no aprendizado de máquina para melhorar a precisão e a estabilidade dos modelos preditivos. A ideia principal é reduzir a variabilidade e evitar o overfitting combinando vários modelos base, geralmente do mesmo tipo.\n",
    "\n",
    "**Ex. Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eis um resumo dos principais passos e conceitos por trás do Bagging:\n",
    "\n",
    "1. **Amostragem com reposição**: O Bagging começa criando várias amostras bootstrap a partir do banco de dados original. Cada amostra é gerada aleatoriamente com reposição, o que significa que algumas observações podem aparecer mais de uma vez na mesma amostra, enquanto outras podem não aparecer.\n",
    "2. **Treinamento independente de modelos**: Para cada amostra bootstrap, um modelo base é treinado separadamente. Normalmente, esses modelos são do mesmo tipo, como decision trees, mas isso não é uma regra fixa.\n",
    "3. **Agregação de resultados**: Após treinar os modelos em diferentes amostras, os resultados são combinados para fazer a previsão final. Em problemas de classificação, isso geralmente envolve votação majoritária (onde se escolhe a classe mais comum entre os modelos). Em problemas de regressão, calcula-se a média das previsões dos modelos.\n",
    "4. **Redução da variância**: O Bagging ajuda a reduzir a variância do modelo, pois a diversidade entre os modelos treinados em diferentes subconjuntos de dados tende a neutralizar os erros específicos de cada modelo. Isso é especialmente eficaz para modelos propensos a overfitting, como os decision trees.\n",
    "\n",
    "**Exemplo comum**: Um exemplo famoso de método Bagging é o Random Forest, no qual vários decision trees são construídos a partir de amostras bootstrap, e cada árvore é treinada com um subconjunto aleatório de características. A previsão final é feita com base na média das previsões das árvores ou por votação majoritária.\n",
    "\n",
    "**Vantagens do Bagging**:\n",
    "\n",
    "- Redução do overfitting.\n",
    "- Melhor generalização do modelo.\n",
    "- Maior robustez e precisão.\n",
    "\n",
    "**Limitações do Bagging**:\n",
    "\n",
    "- Pode ser mais exigente computacionalmente, pois requer a construção de vários modelos.\n",
    "- Não melhora a capacidade do modelo de capturar relações complexas nos dados.\n",
    "\n",
    "O Bagging é uma técnica poderosa e amplamente utilizada em problemas de aprendizado supervisionado para melhorar o desempenho de modelos preditivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é **Boosting**?\n",
    "\n",
    "O boosting é uma das ideias de aprendizado mais poderosas introduzidas nos últimos vinte anos. Inicialmente projetado para problemas de classificação, pode ser estendido com sucesso também para regressão. A motivação por trás do boosting é uma abordagem que combina os resultados de muitos classificadores \"fracos\" para produzir um \"comitê\" poderoso. Nesse sentido, o boosting compartilha semelhanças com o bagging e outras abordagens baseadas em comitês. No entanto, veremos que a conexão é, na melhor das hipóteses, superficial, e o boosting é fundamentalmente diferente.\n",
    "\n",
    "**Ex. AdaBoost (Adaptive Boosting), Gradient Boosting, XGB (eXtreme Gradient Boosting)**\n",
    "\n",
    "*Hastie, Tibshirani, Friedman, 2000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo de **Boosting** é uma técnica de aprendizado de máquina usada para melhorar o desempenho dos modelos preditivos. Sua ideia fundamental é combinar vários modelos fracos para criar um modelo forte. Aqui está uma explicação detalhada de como funciona:\n",
    "\n",
    "### Princípios Básicos\n",
    "\n",
    "1. **Modelos Fracos**: O Boosting começa com um conjunto de modelos fracos, ou seja, modelos que são apenas ligeiramente melhores que o acaso. Estes podem ser simples, como pequenas árvores de decisão (também chamadas de \"stumps\", quando têm apenas um nível de profundidade).\n",
    "2. **Modelo Aditivo**: O Boosting constrói o modelo final de forma incremental, adicionando um modelo fraco de cada vez ao conjunto. Cada novo modelo é treinado para corrigir os erros do modelo combinado anterior.\n",
    "3. **Ponderação dos Erros**: Em cada iteração, os dados que foram classificados erroneamente pelos modelos anteriores recebem um peso maior, para que os modelos subsequentes se concentrem nesses casos difíceis.\n",
    "\n",
    "Esses princípios permitem que o boosting crie um modelo poderoso, mesmo quando os modelos individuais são relativamente fracos. O processo iterativo e a ênfase em corrigir os erros a cada etapa são o que tornam o boosting eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos de Boosting\n",
    "\n",
    "Existem várias variantes de Boosting, incluindo:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting)**:\n",
    "    - Inicialmente atribui um peso igual a todas as observações.\n",
    "    - Após cada iteração, atualiza os pesos das observações: aumenta o peso das que foram classificadas incorretamente e diminui o peso das que foram classificadas corretamente.\n",
    "    - O novo modelo é treinado com o conjunto de dados ponderado.\n",
    "    - A combinação final dos modelos é uma soma ponderada das previsões de cada modelo fraco.\n",
    "2. **Gradient Boosting**:\n",
    "    - Começa com um modelo inicial (geralmente uma previsão simples da média).\n",
    "    - A cada iteração, treina um novo modelo sobre os erros residuais do modelo atual (a diferença entre as previsões do modelo atual e os valores reais).\n",
    "    - O modelo final é a soma dos modelos treinados, com cada modelo adicional tentando melhorar a precisão do modelo anterior, reduzindo os erros residuais.\n",
    "\n",
    "### Aplicações e Vantagens\n",
    "\n",
    "- **Robustez e Precisão**: O Boosting é conhecido por sua alta precisão e robustez. É particularmente eficaz quando se trabalha com modelos fracos, melhorando significativamente o desempenho geral.\n",
    "- **Flexibilidade**: Pode ser aplicado a vários tipos de modelos fracos, não apenas às árvores de decisão.\n",
    "- **Minimização dos Erros**: Através da atenção iterativa aos erros, o Boosting pode lidar bem com dados ruidosos e complexos.\n",
    "\n",
    "### Considerações\n",
    "\n",
    "- **Overfitting**: Se não ajustado corretamente, o Boosting pode se sobreajustar aos dados de treinamento. Para evitar isso, técnicas de regularização são frequentemente utilizadas, como o redimensionamento dos pesos dos modelos adicionais.\n",
    "- **Tempo de Computação**: O processo iterativo e o treinamento múltiplo de modelos podem ser computacionalmente caros e demorados.\n",
    "\n",
    "### Implementações Populares\n",
    "\n",
    "- **Scikit-learn**: Fornece implementações de AdaBoost e Gradient Boosting.\n",
    "- **XGBoost**: Uma biblioteca de Gradient Boosting altamente otimizada que oferece excelente desempenho e escalabilidade.\n",
    "- **LightGBM**: Outra biblioteca de Gradient Boosting, projetada para ser eficiente e rápida.\n",
    "- **CatBoost**: Otimizada para dados categóricos e muito utilizada em competições de machine learning.\n",
    "\n",
    "Em resumo, o Boosting é uma técnica poderosa de machine learning que melhora a precisão dos modelos combinando múltiplos modelos fracos, focando iterativamente nos erros para construir um modelo forte e preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação entre os algoritmos de cada modelo\n",
    "\n",
    "Aqui está uma tabela que resume as principais aplicações de Random Forest, AdaBoost, Gradient Boosting Machine (GBM) e XGBoost:\n",
    "\n",
    "| **Algoritmo** | **Principais Aplicações** |\n",
    "| --- | --- |\n",
    "| **Random Forest** | - Classificação de e-mails (spam/não spam) **/////** - Previsão de risco de crédito **/////** - Detecção de anomalias nos dados **/////** - Previsão de diagnósticos médicos **/////** - Classificação de dados biológicos |\n",
    "| **AdaBoost** | - Detecção de rostos em imagens **/////** - Classificação de dados biomédicos **/////** - Detecção de objetos em vídeos de vigilância **/////** - Previsão de reincidência de pacientes **/////** - Classificação de imagens de satélite |\n",
    "| **Gradient Boosting Machine (GBM)** | - Previsão da taxa de clique para anúncios online **/////** - Análise de sentimentos nas redes sociais **/////** - Previsão da demanda de energia elétrica **/////** - Análise de séries temporais financeiras **/////** - Previsão de comportamentos de usuários online |\n",
    "| **XGBoost (Extreme Gradient Boosting)** | - Previsão de preços de ações **/////** - Previsão de churn de clientes **/////** - Classificação de tweets do Twitter **/////** - Previsão da rentabilidade do cliente **/////** - Previsão de sobrevida de pacientes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está a tabela que compara as principais diferenças entre Random Forest, AdaBoost, Gradient Boosting Machine (GBM) e XGBoost:\n",
    "\n",
    "| **Características** | **Random Forest** | **AdaBoost** | **Gradient Boosting Machine (GBM)** | **XGBoost (Extreme Gradient Boosting)** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| **Tipo de Modelo** | Ensemble de Árvores de Decisão | Ensemble de Modelos (geralmente árvores de decisão) | Ensemble de Modelos (geralmente árvores de decisão) | Ensemble de Modelos (geralmente árvores de decisão) |\n",
    "| **Método de Ensemble** | Bagging (Bootstrap Aggregating) | Boosting | Boosting | Boosting |\n",
    "| **Combinação das Previsões** | Média das previsões das árvores | Soma ponderada dos modelos sequenciais | Soma dos modelos sequenciais | Soma dos modelos sequenciais |\n",
    "| **Peso das Árvores** | Cada árvore tem o mesmo peso | Os modelos são ponderados com base no desempenho | Os modelos são ponderados com base no desempenho | Os modelos são ponderados com base no desempenho |\n",
    "| **Construção das Árvores** | Árvores construídas independentemente com amostragem bootstrap | Modelos construídos sequencialmente, corrigindo os erros dos modelos anteriores | Modelos construídos sequencialmente, corrigindo os erros dos modelos anteriores | Modelos construídos sequencialmente, corrigindo os erros dos modelos anteriores |\n",
    "| **Overfitting** | Reduzido devido à média dos resultados | Pode ocorrer overfitting se não bem ajustado | Pode ocorrer overfitting se não bem ajustado | Reduzido devido à regularização avançada e pruning |\n",
    "| **Velocidade de Treinamento** | Rápido (pode ser paralelizado) | Lento | Lento | Rápido (otimizado para eficiência e pode ser paralelizado) |\n",
    "| **Velocidade de Predição** | Rápido | Rápido | Lento | Rápido |\n",
    "| **Robustez a Dados Ruidosos** | Alta | Baixa sem regularização | Moderada | Alta |\n",
    "| **Gestão de Valores Faltantes** | Bem gerido (árvores independentes) | Não bem gerido | Não bem gerido | Bem gerido (suporte integrado) |\n",
    "| **Interpretabilidade** | Moderada (difícil de interpretar muitas árvores) | Baixa | Baixa | Moderada |\n",
    "| **Principais Parâmetros de Ajuste** | Número de árvores, profundidade das árvores | Número de modelos, learning rate | Número de modelos, learning rate, profundidade das árvores | Número de modelos, learning rate, profundidade das árvores, parâmetros de regularização |\n",
    "| **Principais Vantagens** | Robusto, reduz overfitting, simples de usar | Boas performances com poucos modelos | Alta acurácia, otimizado | Alta acurácia, muito eficiente, regularização avançada |\n",
    "| **Principais Desvantagens** | Nem sempre é o mais preciso, interpretabilidade limitada | Sensível a dados ruidosos, requer bom ajuste | Lento para treinar, complexo de ajustar | Complexo de ajustar, requer recursos computacionais |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tabela fornece uma visão geral das principais diferenças entre esses quatro populares algoritmos de aprendizado de máquina ensemble, cada um com seus próprios pontos fortes e fracos, dependendo da aplicação específica.\n",
    "\n",
    "Aqui estão cinco aplicações reais nas quais o **Random Forest**, **AdaBoost**, **Gradient Boosting Machine (GBM)** e **XGBoost** são preferidos em relação aos outros:\n",
    "\n",
    "1. **Random Forest**:\n",
    "    - **Aplicação 1**: Classificação de e-mails como spam ou não spam.\n",
    "    - **Motivo**: A capacidade de lidar com grandes conjuntos de características e a resistência ao overfitting tornam o Random Forest ideal para a classificação de dados textuais complexos, como e-mails.\n",
    "    - **Aplicação 2**: Previsão de risco de crédito para empréstimos bancários.\n",
    "    - **Motivo**: O Random Forest é eficaz em lidar com dados heterogêneos, como histórico de crédito, renda, ocupação, etc., fornecendo uma avaliação robusta e equilibrada do risco.\n",
    "2. **AdaBoost**:\n",
    "    - **Aplicação 1**: Detecção de rostos em imagens para biometria.\n",
    "    - **Motivo**: O AdaBoost é adequado para melhorar a precisão na detecção de características complexas e variáveis, como rostos, em contextos de segurança e identificação.\n",
    "    - **Aplicação 2**: Classificação de dados biomédicos para diagnóstico de doenças.\n",
    "    - **Motivo**: O AdaBoost é útil quando é necessária alta precisão na classificação de dados de natureza variada (como imagens de exames, dados genômicos) para fornecer diagnósticos precisos e rápidos.\n",
    "3. **Gradient Boosting Machine (GBM)**:\n",
    "    - **Aplicação 1**: Previsão da taxa de cliques para anúncios publicitários online.\n",
    "    - **Motivo**: O GBM é eficaz em combinar informações de diferentes fontes (como dados demográficos, comportamentais e de navegação na web) para melhorar a precisão na previsão do comportamento dos usuários.\n",
    "    - **Aplicação 2**: Análise de sentimentos nas redes sociais para monitorar a percepção da marca.\n",
    "    - **Motivo**: O GBM é adequado para analisar grandes volumes de texto não estruturado de redes sociais, a fim de identificar tendências de opinião e sentimentos em relação a uma marca ou produto.\n",
    "4. **XGBoost (Extreme Gradient Boosting)**:\n",
    "    - **Aplicação 1**: Previsão da demanda de energia elétrica para o planejamento da distribuição de energia.\n",
    "    - **Motivo**: O XGBoost é eficiente ao lidar com dados temporais complexos e variáveis, integrando parâmetros de regularização para prever com precisão a demanda futura de energia.\n",
    "    - **Aplicação 2**: Previsão dos preços das ações para o trading algorítmico.\n",
    "    - **Motivo**: O XGBoost é ideal para a análise de grandes conjuntos de dados financeiros e a previsão dos movimentos dos preços das ações, utilizando características como tendências históricas e indicadores técnicos.\n",
    "\n",
    "Cada aplicação destaca como esses algoritmos são usados para diferentes fins em contextos reais, aproveitando suas características únicas de gerenciamento de dados, robustez e capacidade de adaptação às necessidades específicas de análise e previsão."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
