{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques\n",
    "Os algoritmos de ensemble são técnicas de machine learning que combinam vários modelos base para melhorar o desempenho geral do sistema. A ideia é que, ao agregar as previsões de diferentes modelos, obtém-se uma estimativa mais robusta e precisa em comparação com aquela fornecida por um único modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diferentes tipos de algoritmos de ensemble, mas os principais são:\n",
    "\n",
    "- **Bagging (Bootstrap Aggregating)**:\n",
    "    - **Princípio**: Constrói vários modelos base em subconjuntos aleatórios dos dados de treinamento. Cada subconjunto é obtido usando o método bootstrap (amostragem com reposição).\n",
    "    - **Modelos Típicos**: Random Forest é um exemplo popular de bagging, utilizando decision trees como modelo base.\n",
    "    - **Vantagem**: Reduz a variância do modelo, melhorando a estabilidade e a precisão sem aumentar significativamente o bias.\n",
    "- **Boosting**:\n",
    "    - **Princípio**: Constrói os modelos base um de cada vez, com cada modelo focando nos erros dos modelos anteriores. Os modelos são treinados em sequência, e cada um melhora o anterior.\n",
    "    - **Modelos Típicos**: AdaBoost, Gradient Boosting, XGBoost, LightGBM e CatBoost.\n",
    "    - **Vantagem**: Melhora a precisão reduzindo tanto o bias quanto a variância. É particularmente eficaz em problemas de classificação e regressão.\n",
    "- **Stacking (Stacked Generalization)**:\n",
    "    - **Princípio**: Combina diferentes modelos base em um nível superior por meio de um meta-modelo. Os modelos base produzem suas previsões, que são usadas como entrada para o meta-modelo.\n",
    "    - **Modelo Típico**: Um exemplo é um classificador linear que usa as saídas de vários modelos base como entrada.\n",
    "    - **Vantagem**: Permite aproveitar os pontos fortes de diferentes modelos para obter previsões mais precisas.\n",
    "- **Blending**:\n",
    "    - **Princípio**: Similar ao stacking, mas utiliza um conjunto de validação separado para treinar o meta-modelo. Não usa o conjunto de teste na criação do modelo.\n",
    "    - **Vantagem**: É mais simples de implementar em comparação com o stacking e reduz o risco de overfitting no conjunto de teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens dos Algoritmos de Ensemble:\n",
    "\n",
    "- **Melhoria da Precisão**: Ao agregar vários modelos, tende-se a reduzir o erro global.\n",
    "- **Robustez**: Os modelos de ensemble são menos suscetíveis ao overfitting em comparação com modelos individuais.\n",
    "- **Flexibilidade**: Podem combinar modelos de diferentes tipos, aproveitando suas características complementares.\n",
    "\n",
    "### Desvantagens:\n",
    "\n",
    "- **Complexidade Computacional**: Os algoritmos de ensemble podem ser mais complexos e demandar mais recursos computacionais.\n",
    "- **Dificuldade na Interpretabilidade**: Como combinam vários modelos, pode ser difícil interpretar o comportamento geral do sistema.\n",
    "\n",
    "Em resumo, os algoritmos de ensemble são uma estratégia poderosa para melhorar o desempenho dos modelos de machine learning, aproveitando a diversidade e a sinergia entre diferentes modelos base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferença entre algoritmos de Bagging e Boosting:\n",
    "\n",
    "Os métodos de Bagging e Boosting são ambos técnicas de ensemble learning utilizadas para melhorar o desempenho dos modelos de machine learning. No entanto, **diferem significativamente na forma como combinam os modelos base** (frequentemente chamados de \"weak learners\" ou \"base learners\") para criar um modelo mais robusto e preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Processo de Treinamento**: O **Bagging** treina modelos base **de forma independente e paralela**, enquanto o **Boosting** treina modelos base **de forma sequencial**, com cada modelo focando nos pontos fracos dos modelos anteriores.\n",
    "- **Atualização de Pesos**: O **Boosting atualiza os pesos dos exemplos durante o treinamento** para se concentrar nos erros, enquanto o **Bagging trata todos os subconjuntos de dados de treinamento de forma equivalente.**\n",
    "- **Combinação de Modelos**: O **Bagging** combina modelos base por meio de uma **agregação simples**, enquanto o **Boosting** combina modelos base **de forma que cada um contribua com um peso variável**, geralmente baseado em seu desempenho.\n",
    "\n",
    "Em resumo, embora ambos os métodos se baseiem na ideia de combinar modelos base para melhorar o desempenho do modelo final, suas abordagens para o amostramento de dados, o treinamento dos modelos e o tratamento de erros os tornam distintos na prática e adequados para diferentes contextos de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é Bagging?\n",
    "\n",
    "O Bagging, abreviação de \"Bootstrap Aggregating\", é uma técnica de ensemble utilizada no aprendizado de máquina para melhorar a precisão e a estabilidade dos modelos preditivos. A ideia principal é reduzir a variabilidade e evitar o overfitting combinando vários modelos base, geralmente do mesmo tipo.\n",
    "\n",
    "**Ex. Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eis um resumo dos principais passos e conceitos por trás do Bagging:\n",
    "\n",
    "1. **Amostragem com reposição**: O Bagging começa criando várias amostras bootstrap a partir do banco de dados original. Cada amostra é gerada aleatoriamente com reposição, o que significa que algumas observações podem aparecer mais de uma vez na mesma amostra, enquanto outras podem não aparecer.\n",
    "2. **Treinamento independente de modelos**: Para cada amostra bootstrap, um modelo base é treinado separadamente. Normalmente, esses modelos são do mesmo tipo, como decision trees, mas isso não é uma regra fixa.\n",
    "3. **Agregação de resultados**: Após treinar os modelos em diferentes amostras, os resultados são combinados para fazer a previsão final. Em problemas de classificação, isso geralmente envolve votação majoritária (onde se escolhe a classe mais comum entre os modelos). Em problemas de regressão, calcula-se a média das previsões dos modelos.\n",
    "4. **Redução da variância**: O Bagging ajuda a reduzir a variância do modelo, pois a diversidade entre os modelos treinados em diferentes subconjuntos de dados tende a neutralizar os erros específicos de cada modelo. Isso é especialmente eficaz para modelos propensos a overfitting, como os decision trees.\n",
    "\n",
    "**Exemplo comum**: Um exemplo famoso de método Bagging é o Random Forest, no qual vários decision trees são construídos a partir de amostras bootstrap, e cada árvore é treinada com um subconjunto aleatório de características. A previsão final é feita com base na média das previsões das árvores ou por votação majoritária.\n",
    "\n",
    "**Vantagens do Bagging**:\n",
    "\n",
    "- Redução do overfitting.\n",
    "- Melhor generalização do modelo.\n",
    "- Maior robustez e precisão.\n",
    "\n",
    "**Limitações do Bagging**:\n",
    "\n",
    "- Pode ser mais exigente computacionalmente, pois requer a construção de vários modelos.\n",
    "- Não melhora a capacidade do modelo de capturar relações complexas nos dados.\n",
    "\n",
    "O Bagging é uma técnica poderosa e amplamente utilizada em problemas de aprendizado supervisionado para melhorar o desempenho de modelos preditivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é **Boosting**?\n",
    "\n",
    "O boosting é uma das ideias de aprendizado mais poderosas introduzidas nos últimos vinte anos. Inicialmente projetado para problemas de classificação, pode ser estendido com sucesso também para regressão. A motivação por trás do boosting é uma abordagem que combina os resultados de muitos classificadores \"fracos\" para produzir um \"comitê\" poderoso. Nesse sentido, o boosting compartilha semelhanças com o bagging e outras abordagens baseadas em comitês. No entanto, veremos que a conexão é, na melhor das hipóteses, superficial, e o boosting é fundamentalmente diferente.\n",
    "\n",
    "**Ex. AdaBoost (Adaptive Boosting), Gradient Boosting, XGB (eXtreme Gradient Boosting)**\n",
    "\n",
    "*Hastie, Tibshirani, Friedman, 2000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo de **Boosting** é uma técnica de aprendizado de máquina usada para melhorar o desempenho dos modelos preditivos. Sua ideia fundamental é combinar vários modelos fracos para criar um modelo forte. Aqui está uma explicação detalhada de como funciona:\n",
    "\n",
    "### Princípios Básicos\n",
    "\n",
    "1. **Modelos Fracos**: O Boosting começa com um conjunto de modelos fracos, ou seja, modelos que são apenas ligeiramente melhores que o acaso. Estes podem ser simples, como pequenas árvores de decisão (também chamadas de \"stumps\", quando têm apenas um nível de profundidade).\n",
    "2. **Modelo Aditivo**: O Boosting constrói o modelo final de forma incremental, adicionando um modelo fraco de cada vez ao conjunto. Cada novo modelo é treinado para corrigir os erros do modelo combinado anterior.\n",
    "3. **Ponderação dos Erros**: Em cada iteração, os dados que foram classificados erroneamente pelos modelos anteriores recebem um peso maior, para que os modelos subsequentes se concentrem nesses casos difíceis.\n",
    "\n",
    "Esses princípios permitem que o boosting crie um modelo poderoso, mesmo quando os modelos individuais são relativamente fracos. O processo iterativo e a ênfase em corrigir os erros a cada etapa são o que tornam o boosting eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos de Boosting\n",
    "\n",
    "Existem várias variantes de Boosting, incluindo:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting)**:\n",
    "    - Inicialmente atribui um peso igual a todas as observações.\n",
    "    - Após cada iteração, atualiza os pesos das observações: aumenta o peso das que foram classificadas incorretamente e diminui o peso das que foram classificadas corretamente.\n",
    "    - O novo modelo é treinado com o conjunto de dados ponderado.\n",
    "    - A combinação final dos modelos é uma soma ponderada das previsões de cada modelo fraco.\n",
    "2. **Gradient Boosting**:\n",
    "    - Começa com um modelo inicial (geralmente uma previsão simples da média).\n",
    "    - A cada iteração, treina um novo modelo sobre os erros residuais do modelo atual (a diferença entre as previsões do modelo atual e os valores reais).\n",
    "    - O modelo final é a soma dos modelos treinados, com cada modelo adicional tentando melhorar a precisão do modelo anterior, reduzindo os erros residuais.\n",
    "\n",
    "### Aplicações e Vantagens\n",
    "\n",
    "- **Robustez e Precisão**: O Boosting é conhecido por sua alta precisão e robustez. É particularmente eficaz quando se trabalha com modelos fracos, melhorando significativamente o desempenho geral.\n",
    "- **Flexibilidade**: Pode ser aplicado a vários tipos de modelos fracos, não apenas às árvores de decisão.\n",
    "- **Minimização dos Erros**: Através da atenção iterativa aos erros, o Boosting pode lidar bem com dados ruidosos e complexos.\n",
    "\n",
    "### Considerações\n",
    "\n",
    "- **Overfitting**: Se não ajustado corretamente, o Boosting pode se sobreajustar aos dados de treinamento. Para evitar isso, técnicas de regularização são frequentemente utilizadas, como o redimensionamento dos pesos dos modelos adicionais.\n",
    "- **Tempo de Computação**: O processo iterativo e o treinamento múltiplo de modelos podem ser computacionalmente caros e demorados.\n",
    "\n",
    "### Implementações Populares\n",
    "\n",
    "- **Scikit-learn**: Fornece implementações de AdaBoost e Gradient Boosting.\n",
    "- **XGBoost**: Uma biblioteca de Gradient Boosting altamente otimizada que oferece excelente desempenho e escalabilidade.\n",
    "- **LightGBM**: Outra biblioteca de Gradient Boosting, projetada para ser eficiente e rápida.\n",
    "- **CatBoost**: Otimizada para dados categóricos e muito utilizada em competições de machine learning.\n",
    "\n",
    "Em resumo, o Boosting é uma técnica poderosa de machine learning que melhora a precisão dos modelos combinando múltiplos modelos fracos, focando iterativamente nos erros para construir um modelo forte e preciso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
