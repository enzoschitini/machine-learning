{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine - GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **O que é o** Gradient Boosting **Machine - GBM:**\n",
    "\n",
    "O Gradient Boosting é uma técnica poderosa de aprendizado de máquina usada principalmente para tarefas de regressão e classificação. Ela combina a previsibilidade de vários modelos simples (fracos) para criar um modelo final forte. Aqui está uma explicação detalhada de como ela funciona:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceitos Básicos\n",
    "\n",
    "1. **Boosting**:  \n",
    "    - O Boosting é uma técnica que combina diversos weak predictors para criar um strong predictor. A ideia é construir modelos em sequência, onde cada modelo subsequente tenta corrigir os erros do modelo anterior.\n",
    "\n",
    "2. **Gradient Descent**:  \n",
    "    - É um algoritmo de otimização usado para encontrar os mínimos de uma função. No Gradient Boosting, ele é utilizado para minimizar a loss ajustando os parâmetros dos modelos.\n",
    "\n",
    "### Como Funciona o Gradient Boosting\n",
    "\n",
    "1. **Inicialização**:  \n",
    "    - O processo começa com um modelo base \\( f_0 \\), que pode ser um preditor simples, como a média dos valores de saída no caso de regressão.\n",
    "\n",
    "2. **Construção de Modelos Sequenciais**:  \n",
    "    - Uma série de \\( M \\) weak models \\( f_1, f_2, \\dots, f_M \\) é construída iterativamente.  \n",
    "    - A cada iteração \\( m \\), um novo modelo \\( f_m \\) é adicionado ao modelo combinado para reduzir o erro residual dos modelos anteriores.  \n",
    "\n",
    "3. **Cálculo dos Resíduos**:  \n",
    "    - A cada passo, os resíduos são calculados como a diferença entre as previsões atuais e os valores reais. Os resíduos representam os erros cometidos pelo modelo atual.  \n",
    "    - Resíduo \\( r_i^{(m)} \\) para a iteração \\( m \\):  \n",
    "      \\[\n",
    "      r_i^{(m)} = y_i - f_{m-1}(x_i)\n",
    "      \\]\n",
    "      Onde \\( y_i \\) é o valor real e \\( f_{m-1}(x_i) \\) é a previsão do modelo corrente.\n",
    "\n",
    "4. **Treinamento do Novo Modelo**:  \n",
    "    - Um novo modelo \\( f_m \\) é treinado para prever os resíduos calculados no passo anterior.\n",
    "\n",
    "5. **Atualização do Modelo**:  \n",
    "    - O modelo atual é atualizado combinando o modelo anterior com o novo modelo ponderado por um learning rate \\( \\eta \\):  \n",
    "      \\[\n",
    "      f_m(x) = f_{m-1}(x) + \\eta \\cdot f_m(x)\n",
    "      \\]\n",
    "      \\( \\eta \\) é um parâmetro que controla a contribuição de cada novo modelo (geralmente um valor pequeno para evitar overfitting).\n",
    "\n",
    "6. **Iteração**:  \n",
    "    - O processo é repetido até que o número de modelos \\( M \\) seja atingido ou até que o erro fique abaixo de um limite predefinido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens do Gradient Boosting\n",
    "\n",
    "- **Precisão**: Geralmente produz modelos muito precisos.\n",
    "- **Flexibilidade**: Pode ser usado para uma ampla gama de funções de perda e com vários tipos de weak predictors.\n",
    "- **Gerenciamento de Features Heterogêneas**: É capaz de lidar com diferentes tipos de variáveis (numéricas, categóricas, etc.).\n",
    "\n",
    "### Desvantagens do Gradient Boosting\n",
    "\n",
    "- **Tempo de Treinamento**: Pode ser lento para treinar, especialmente com grandes datasets.\n",
    "- **Sensível a Dados Ruidosos**: Pode overfit se não for devidamente regularizado (regularized).\n",
    "\n",
    "### Melhorias e Variações\n",
    "\n",
    "- **Gradient Boosting Machines (GBM)**: Uma biblioteca popular que implementa gradient boosting.\n",
    "- **XGBoost**: Uma biblioteca otimizada para eficiência e velocidade.\n",
    "- **LightGBM**: Projetada para ser rápida e escalável para grandes datasets.\n",
    "- **CatBoost**: Otimizada para dados categóricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Diferenças entre AdaBoost e Gradient Boosting Machine:**\n",
    "\n",
    "Os pontos abaixo destacam as principais diferenças nos métodos de treinamento, no gerenciamento dos pesos dos exemplos, nos tipos de modelos base utilizados, nas funções objetivo otimizadas e nas características gerais de complexidade, flexibilidade e velocidade de treinamento.\n",
    "\n",
    "AdaBoost e Gradient Boosting Machine (GBM) são ambos métodos de ensemble que combinam vários weak predictors para construir um modelo preditivo mais robusto. No entanto, existem diferenças fundamentais entre as duas abordagens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/enzoschitini/machine-learning/refs/heads/Ensemble-Techniques/img/GBM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Característica | AdaBoost | Gradient Boosting Machine |\n",
    "| --- | --- | --- |\n",
    "| **Processo de treinamento** | Treina weak predictors ajustando os pesos dos erros | Treina weak predictors sequencialmente minimizando os resíduos do modelo anterior |\n",
    "| **Gestão dos pesos dos exemplos** | Aumenta o peso dos exemplos classificados erroneamente | Não modifica diretamente os pesos, foca nos resíduos |\n",
    "| **Tipo de modelos base** | Geralmente árvores de decisão com profundidade limitada | Árvores de decisão, mas pode usar modelos mais complexos |\n",
    "| **Função objetivo** | Minimiza o erro de classificação | Minimiza uma função de perda (deviança para regressão, entropia cruzada para classificação) |\n",
    "| **Complexidade e flexibilidade** | Menos flexível, indicado para modelos simples | Mais flexível, adequado para modelos mais complexos |\n",
    "| **Sensibilidade ao overfitting** | Mais sensível ao overfitting se não regularizado | Menos sensível ao overfitting se bem regularizado |\n",
    "| **Velocidade de treinamento** | Geralmente mais rápido | Geralmente mais lento |\n",
    "| **Floresta** | Floresta de Stumps | Floresta de Árvores |\n",
    "| **Primeiro passo** | O primeiro passo é um Stump | O primeiro passo é a média de YY |\n",
    "| **Peso dos modelos** | Os modelos têm pesos diferentes | Todos os modelos são multiplicados por η\\eta |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hiperparâmetros Importantes no Gradient Boosting Machine (GBM)\n",
    "\n",
    "---\n",
    "\n",
    "No Gradient Boosting Machine (GBM), o ajuste correto dos hiperparâmetros é crucial para obter um modelo ideal. A seguir, estão os 5 hiperparâmetros mais importantes:\n",
    "\n",
    "1. **Número de Árvores (n_estimators)**:\n",
    "    - Determina o número total de árvores de decisão (modelos fracos) no processo de boosting.\n",
    "    - **Efeito**: Um número maior pode melhorar a precisão, mas aumenta o risco de overfitting e o tempo de treinamento.\n",
    "    - **Sugestão**: Use valores altos combinados com uma baixa `learning_rate` para melhorar a generalização.\n",
    "2. **Taxa de Aprendizado (learning_rate)**:\n",
    "    - Controla a velocidade com que o modelo aprende, reduzindo a influência de cada árvore.\n",
    "    - **Efeito**: Valores mais baixos requerem mais árvores (`n_estimators`), mas aumentam a robustez contra overfitting.\n",
    "    - **Sugestão**: Comece com valores baixos (ex.: 0.01 ou 0.1) e ajuste o número de árvores em conformidade.\n",
    "3. **Profundidade Máxima das Árvores (max_depth)**:\n",
    "    - Especifica a profundidade máxima das árvores de decisão.\n",
    "    - **Efeito**: Árvores mais profundas capturam relações mais complexas, mas aumentam o risco de overfitting.\n",
    "    - **Sugestão**: Para datasets simples ou com ruído, utilize profundidades menores (ex.: 3-5).\n",
    "4. **Subamostragem (subsample)**:\n",
    "    - Indica a porcentagem de dados usada para construir cada árvore.\n",
    "    - **Efeito**: Valores menores que 1.0 introduzem aleatoriedade e reduzem a variância, melhorando a capacidade de generalização.\n",
    "    - **Sugestão**: Teste valores entre 0.7 e 0.9 para resultados equilibrados.\n",
    "5. **Número Mínimo de Amostras em uma Folha (min_samples_leaf)**:\n",
    "    - Representa o número mínimo de dados necessários para formar uma folha em uma árvore.\n",
    "    - **Efeito**: Valores mais altos simplificam os modelos, reduzindo o risco de overfitting.\n",
    "    - **Sugestão**: Use valores entre 10 e 50 para datasets de tamanho médio.\n",
    "\n",
    "### Outros Hiperparâmetros Importantes\n",
    "\n",
    "Além desses cinco, outros hiperparâmetros, como **max_features** (número máximo de features consideradas para a divisão) e **regularization terms** (L1/L2), podem influenciar significativamente o desempenho. No entanto, os cinco listados representam os mais críticos para um ajuste inicial eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Diferença Principal entre Stochastic Gradient Boosting Machine e Gradient Boosting Machine**\n",
    "\n",
    "O termo **\"Stochastic Gradient Boosting\" (SGB)** refere-se a uma **variante do Gradient Boosting Machine** (GBM), proposta por Jerome Friedman. A maior diferença entre o tradicional Gradient Boosting e o Stochastic Gradient Boosting está na introdução de aleatoriedade durante o treinamento dos modelos base (normalmente árvores de decisão) em cada iteração do processo de boosting.\n",
    "\n",
    "### Resumo das Diferenças\n",
    "\n",
    "- **Uso do Dataset**: O GBM utiliza o dataset completo em cada iteração, enquanto o SGBM utiliza apenas um subconjunto aleatório dos dados em cada iteração.\n",
    "- **Velocidade de Treinamento**: O SGBM tende a ser mais rápido devido ao amostramento de subconjuntos menores.\n",
    "- **Robustez e Generalização**: O SGBM pode ser mais robusto e generalizar melhor, reduzindo o risco de overfitting em comparação ao GBM.\n",
    "\n",
    "Em resumo, a principal diferença entre **Stochastic Gradient Boosting Machine** e **Gradient Boosting Machine** está no uso de amostragem aleatória pelo SGBM, que introduz aleatoriedade no treinamento, tornando o algoritmo mais eficiente e frequentemente mais robusto do que o GBM tradicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Característica** | **Gradient Boosting Machine (GBM)** | **Stochastic Gradient Boosting Machine (SGBM)** |\n",
    "| --- | --- | --- |\n",
    "| **Definição** | Algoritmo de boosting que cria um modelo preditivo treinando sequencialmente modelos fracos em todo o dataset. | Variante do GBM que utiliza amostragem aleatória de um subconjunto do dataset em cada iteração. |\n",
    "| **Amostragem de Dados** | Usa o dataset completo para treinar cada nova árvore. | Usa um subconjunto aleatório do dataset para treinar cada nova árvore (*stochastic subsampling*). |\n",
    "| **Eficiência** | Relativamente lento, pois cada iteração utiliza o dataset completo. | Mais rápido, já que cada iteração usa apenas uma parte do dataset. |\n",
    "| **Robustez e Generalização** | Maior risco de overfitting em datasets com ruído. | Maior robustez e melhor generalização, reduzindo o risco de overfitting. |\n",
    "| **Objetivo do Treinamento** | Minimiza uma função de perda treinando novos modelos nos resíduos dos erros anteriores. | Mesmo objetivo do GBM, mas com adição de aleatoriedade para reduzir a variância e melhorar a robustez. |\n",
    "| **Principal Vantagem** | Poderoso para modelar padrões complexos nos dados. | Mais robusto e rápido, reduzindo a variância e melhorando a capacidade de generalização. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As duas técnicas de machine learning mencionadas, **Stochastic Gradient Boosting Machine (SGBM)** e **Gradient Boosting Machine (GBM)**, são variantes de algoritmos de boosting usados para melhorar a precisão de modelos preditivos, combinando uma série de modelos fracos para criar um modelo forte. Aqui estão as principais diferenças entre as duas:\n",
    "\n",
    "### Gradient Boosting Machine (GBM)\n",
    "\n",
    "1. **Definição**:\n",
    "    - O GBM é um algoritmo de aprendizado supervisionado que cria um modelo preditivo através de uma série de modelos fracos, geralmente árvores de decisão, treinados sequencialmente.\n",
    "    - O objetivo é minimizar uma função de perda, como o erro quadrático médio para regressão ou a log-loss para classificação.\n",
    "2. **Abordagem de Treinamento**:\n",
    "    - Em cada iteração, uma nova árvore é treinada sobre os resíduos do erro do modelo anterior.\n",
    "    - Cada novo modelo tenta corrigir os erros cometidos pelo modelo anterior, melhorando gradualmente o desempenho geral.\n",
    "3. **Amostragem**:\n",
    "    - O GBM usa o dataset completo para treinar cada nova árvore, sem amostragem aleatória dos dados.\n",
    "4. **Eficiência e Velocidade**:\n",
    "    - O GBM pode ser relativamente lento, pois utiliza o dataset completo para cada iteração.\n",
    "\n",
    "### Stochastic Gradient Boosting Machine (SGBM)\n",
    "\n",
    "1. **Definição**:\n",
    "    - O SGBM é uma variante do GBM que introduz aleatoriedade no processo de treinamento, utilizando amostragem aleatória de uma parte do dataset em cada iteração.\n",
    "2. **Abordagem de Treinamento**:\n",
    "    - Semelhante ao GBM, cada novo modelo tenta corrigir os erros do modelo anterior. No entanto, a principal diferença é que cada árvore é treinada sobre um subconjunto aleatório do dataset.\n",
    "    - Essa amostragem é conhecida como \"stochastic subsampling\" ou \"bagging\".\n",
    "3. **Amostragem**:\n",
    "    - Em vez de usar o dataset completo, o SGBM seleciona aleatoriamente uma fração do dataset (geralmente 50% ou menos) para treinar cada nova árvore.\n",
    "    - Essa abordagem pode reduzir a variância e prevenir overfitting, tornando o modelo mais robusto.\n",
    "4. **Eficiência e Velocidade**:\n",
    "    - O SGBM tende a ser mais rápido que o GBM, pois cada árvore é treinada sobre um subconjunto menor de dados.\n",
    "    - A redução da variância pode levar a uma melhor generalização, especialmente em datasets ruidosos ou com muitos outliers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
