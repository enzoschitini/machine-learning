{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **O que é AdaBoost:**\n",
    "\n",
    "AdaBoost, abreviação de \"Adaptive Boosting\", é um algoritmo de aprendizado de máquina que faz parte da família dos algoritmos de boosting. O boosting é uma técnica de ensemble learning que combina a previsão de múltiplos modelos fracos (ou seja, modelos que têm um desempenho ligeiramente melhor que o acaso) para criar um modelo forte que obtém uma precisão geral melhor.\n",
    "\n",
    "1. **Inicialização dos Pesos**: No início, o AdaBoost atribui pesos iguais a todos os exemplos do conjunto de treinamento. Esses pesos representam a importância relativa de cada exemplo no conjunto de dados.\n",
    "2. **Construção de Modelos Fracos**: O AdaBoost treina iterativamente uma série de modelos fracos (frequentemente stumps, ou seja, árvores de decisão de profundidade 1). A cada iteração, um novo modelo fraco é treinado usando os pesos atuais dos exemplos.\n",
    "3. **Cálculo do Erro**: Após o treinamento de um modelo fraco, o AdaBoost avalia seu desempenho calculando o erro ponderado, ou seja, a soma dos pesos dos exemplos que foram classificados incorretamente pelo modelo.\n",
    "4. **Atualização dos Pesos**: Os pesos dos exemplos classificados corretamente pelo modelo fraco são reduzidos, enquanto os pesos dos exemplos classificados incorretamente são aumentados. Esse processo garante que os modelos subsequentes se concentrem mais nos exemplos que foram difíceis de classificar nas iterações anteriores.\n",
    "5. **Cálculo do Fator de Voto**: Cada modelo fraco recebe um fator de voto com base em sua precisão. Modelos com melhor desempenho recebem um peso maior no voto final.\n",
    "6. **Combinação dos Modelos Fracos**: Ao final do processo, os modelos fracos são combinados em um ensemble. A previsão final é uma média ponderada das previsões de todos os modelos fracos, onde os pesos são os fatores de voto calculados anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens do AdaBoost\n",
    "\n",
    "- **Simplicidade**: O AdaBoost é relativamente simples de implementar e pode ser aplicado a uma variedade de modelos fracos.\n",
    "- **Flexibilidade**: Pode melhorar significativamente o desempenho de modelos fracos.\n",
    "- **Robustez**: É resistente ao overfitting (sobretreinamento) se o número de iterações for adequado.\n",
    "\n",
    "### Desvantagens do AdaBoost\n",
    "\n",
    "- **Sensibilidade aos Dados Ruídos**: O AdaBoost pode ser muito sensível aos dados ruidosos e aos valores discrepantes, pois tende a se concentrar nos exemplos difíceis.\n",
    "- **Exige uma boa base de modelos fracos**: Se os modelos fracos forem muito fracos, o boosting pode não melhorar suficientemente o desempenho.\n",
    "\n",
    "Em resumo, o AdaBoost é um poderoso algoritmo de boosting que melhora a precisão de um modelo combinando diferentes modelos fracos de maneira adaptativa, concentrando-se mais nos exemplos difíceis de classificar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 diferenças entre Random Forest e AdaBoost:**\n",
    "\n",
    "Random Forest e AdaBoost são ambos algoritmos de aprendizado de conjunto, mas possuem características diferentes. Aqui estão cinco diferenças principais entre eles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/enzoschitini/machine-learning/refs/heads/Ensemble-Techniques/img/AdaBoost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Floresta | Floresta de Árvores | Floresta de Stumps |\n",
    "| --- | --- | --- |\n",
    "| Árvores | Árvores não conectadas entre si | Árvores conectadas entre si |\n",
    "| Resposta das árvores | Têm o mesmo peso | Têm pesos diferentes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Método de construção da árvore:**\n",
    "    - **Random Forest:** cria várias árvores de decisão de forma independente, utilizando uma amostra aleatória (com reposição) do conjunto de dados para cada árvore. Além disso, a cada divisão de nó, é considerada uma amostra aleatória de características.\n",
    "    - **AdaBoost:** cria árvores de decisão em sequência, onde cada árvore tenta corrigir os erros da árvore anterior. Cada árvore é treinada com um conjunto de dados ponderado, dando mais peso às observações classificadas incorretamente pelas árvores anteriores.\n",
    "2. **Combinação de resultados:**\n",
    "    - **Random Forest:** usa o voto da maioria (para classificação) ou a média (para regressão) dos resultados das árvores para tomar a decisão final.\n",
    "    - **AdaBoost:** combina os resultados das árvores, ponderando cada árvore com base em sua precisão. Árvores com melhor desempenho têm maior influência na decisão final.\n",
    "3. **Resistência ao overfitting:**\n",
    "    - **Random Forest:** tende a ser mais resistente ao overfitting devido ao uso de árvores não correlacionadas. A aleatoriedade na seleção de amostras e características ajuda a reduzir a variância.\n",
    "    - **AdaBoost:** pode ser mais suscetível ao overfitting, especialmente se não configurado corretamente. Isso porque ele tenta de maneira agressiva se ajustar aos erros de treinamento.\n",
    "4. **Complexidade e tempo de treinamento:**\n",
    "    - **Random Forest:** pode ser treinado em paralelo, pois as árvores são independentes entre si. Isso pode tornar o treinamento mais rápido em ambientes com recursos de processamento paralelo.\n",
    "    - **AdaBoost:** o treinamento é sequencial, pois cada árvore depende dos erros da anterior, o que pode aumentar o tempo de treinamento em comparação ao Random Forest.\n",
    "5. **Sensibilidade a valores discrepantes e ruídos:**\n",
    "    - **Random Forest:** é relativamente resistente a valores discrepantes e ao ruído no conjunto de dados, pois a aleatoriedade introduzida ajuda a atenuar os efeitos desses elementos.\n",
    "    - **AdaBoost:** é mais sensível a valores discrepantes e ao ruído porque tenta corrigir os erros no conjunto de treinamento, o que pode levar a um ajuste excessivo de valores discrepantes e ruídos.\n",
    "\n",
    "Essas diferenças ilustram como o Random Forest e o AdaBoost abordam o problema de machine learning de maneiras distintas, cada um com suas vantagens e desvantagens específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hiperparâmetros importantes no AdaBoost:\n",
    "\n",
    "No AdaBoost, existem vários hiperparâmetros que podem ser otimizados para melhorar o desempenho do modelo. Aqui estão cinco hiperparâmetros importantes:\n",
    "\n",
    "1. **Número de estimadores (`n_estimators`):**\n",
    "    - Este hiperparâmetro define o número de árvores de decisão (ou outros estimadores base) que serão treinadas em sequência. Um número maior de estimadores pode melhorar o desempenho, mas também aumenta o risco de overfitting e o tempo de treinamento.\n",
    "2. **Taxa de aprendizado (`learning_rate`):**\n",
    "    - A taxa de aprendizado reduz a contribuição de cada árvore. Valores menores de `learning_rate` (combinados com um número maior de estimadores) podem levar a melhores desempenhos, mas aumentam o tempo de treinamento. Os valores típicos variam de 0,01 a 1,0.\n",
    "3. **Tipo de estimador base (`base_estimator`):**\n",
    "    - Define o tipo de estimador base a ser utilizado no conjunto. Normalmente, é utilizado um classificador simples de árvore de decisão (\"DecisionTreeClassifier\" com profundidade 1, conhecido também como \"stump\"), mas é possível usar outros tipos de classificadores.\n",
    "4. **Algoritmo (`algorithm`):**\n",
    "    - Determina o algoritmo de boosting a ser utilizado. Os valores comuns são `SAMME` para classificação multiclasse ou `SAMME.R`, uma variante do SAMME que utiliza probabilidades reais das árvores de decisão.\n",
    "5. **Parâmetros do estimador base:**\n",
    "    - Quando se utiliza um `base_estimator` específico, também é possível ajustar seus hiperparâmetros. Por exemplo, se o `base_estimator` for um `DecisionTreeClassifier`, parâmetros como a profundidade máxima (`max_depth`), o número mínimo de amostras por folha (`min_samples_leaf`) e o critério de divisão (\"criterion\") podem ser importantes para controlar a complexidade e o desempenho das árvores base.\n",
    "\n",
    "A configuração correta desses hiperparâmetros pode ter um impacto significativo no desempenho do AdaBoost, e muitas vezes é necessário experimentar várias combinações para encontrar a configuração ideal para um determinado conjunto de dados."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
