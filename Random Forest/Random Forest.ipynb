{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é Random Forest?\n",
    "\n",
    "Random Forest é um algoritmo de machine learning utilizado tanto para tarefas de classificação quanto de regressão. Ele se baseia na construção de múltiplas árvores de decisão durante o treinamento e na produção de uma classe que é a moda das classes (no caso de classificação) ou a média das previsões (no caso de regressão) das árvores individuais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/enzoschitini/machine-learning/refs/heads/Ensemble-Techniques/img/Random%20Forest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está um resumo de como funciona o Random Forest:\n",
    "\n",
    "1. **Criação de múltiplas árvores de decisão**: O Random Forest cria várias árvores de decisão independentes umas das outras. Cada uma dessas árvores é treinada usando um subconjunto diferente dos dados de treinamento e um subconjunto diferente das funcionalidades disponíveis.\n",
    "2. **Bagging (bootstrap aggregation)**: Para construir cada árvore, o algoritmo utiliza uma técnica chamada \"bootstrap aggregation\" ou \"bagging\". Isso significa que ele seleciona aleatoriamente um subconjunto dos dados de treinamento, com reposição. Isso resulta em amostras de treinamento diferentes para cada árvore, o que ajuda a aumentar a diversidade das árvores.\n",
    "3. **Seleção aleatória das características**: Durante a construção de cada árvore, o algoritmo seleciona aleatoriamente um subconjunto das características em cada nó para determinar a melhor divisão. Isso reduz a correlação entre as árvores, melhorando a precisão e a robustez do modelo final.\n",
    "4. **Combinação dos resultados**: Após a construção de todas as árvores, o Random Forest faz previsões combinando os resultados de todas as árvores. Para tarefas de classificação, a classe final é determinada pela maioria das classes previstas pelas árvores (votação majoritária). Para tarefas de regressão, a previsão final é a média das previsões de todas as árvores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens do Random Forest:\n",
    "\n",
    "- **Alta precisão**: Devido à combinação de várias árvores, o Random Forest tende a ter alta precisão e robustez.\n",
    "- **Resistente ao overfitting**: A técnica de bagging e a seleção aleatória das características ajudam a reduzir o overfitting em relação aos modelos individuais de árvores de decisão.\n",
    "- **Versátil**: Pode ser utilizado tanto para classificação quanto para regressão.\n",
    "- **Trabalha com dados ausentes**: O Random Forest pode lidar com dados ausentes de maneira mais eficaz do que outras técnicas.\n",
    "\n",
    "### Desvantagens do Random Forest:\n",
    "\n",
    "- **Complexidade**: O modelo final pode ser difícil de interpretar devido ao grande número de árvores.\n",
    "- **Tempo de treinamento**: O treinamento pode ser mais lento em comparação com modelos mais simples, especialmente em conjuntos de dados muito grandes.\n",
    "\n",
    "Em resumo, o Random Forest é uma técnica poderosa de aprendizado de máquina que melhora a precisão e a estabilidade das previsões combinando várias árvores de decisão treinadas de forma aleatória e independente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qual é a diferença entre Bagging e Random Forest?\n",
    "\n",
    "Bagging e Random Forest são técnicas de aprendizado de máquina que utilizam a ideia de treinamento de múltiplos modelos e a combinação de seus resultados para melhorar a precisão e a robustez. No entanto, existe uma diferença importante entre elas:\n",
    "\n",
    "### **Bagging** (Bootstrap Aggregating)\n",
    "\n",
    "- **Objetivo**: Bagging é uma técnica de ensemble que visa reduzir a variância de modelos de alto desempenho (como árvores de decisão) treinando múltiplos modelos de forma independente, usando amostras diferentes dos dados de treinamento.\n",
    "- **Como funciona**: Durante o treinamento, o algoritmo cria múltiplas amostras de treinamento com reposição (bootstrap), ou seja, escolhe aleatoriamente os dados com reposição para criar diferentes subconjuntos do conjunto de treinamento. Cada modelo é treinado com um desses subconjuntos e, após o treinamento, os resultados dos modelos são combinados. No caso da classificação, é feito um voto majoritário, e, na regressão, é feita a média das previsões.\n",
    "- **Características**: Bagging pode ser usado com qualquer tipo de modelo, mas é frequentemente aplicado a árvores de decisão.\n",
    "\n",
    "### **Random Forest**\n",
    "\n",
    "- **Objetivo**: Random Forest é uma técnica de ensemble que também utiliza a ideia do Bagging, mas adiciona um passo extra para aumentar a diversidade entre as árvores e melhorar ainda mais a precisão e a robustez do modelo.\n",
    "- **Como funciona**: Além de usar a técnica de Bagging para criar amostras de treinamento com reposição, o Random Forest também realiza **seleção aleatória das características** em cada nó de cada árvore. Ou seja, durante o treinamento de cada árvore, o algoritmo escolhe aleatoriamente um subconjunto das características disponíveis em cada divisão de nó, o que ajuda a reduzir a correlação entre as árvores. Isso resulta em um modelo mais robusto e menos suscetível ao overfitting.\n",
    "- **Características**: O Random Forest é uma implementação específica de Bagging com a adição da seleção aleatória das características, o que o torna mais poderoso para problemas complexos.\n",
    "\n",
    "### **Diferenças principais**\n",
    "\n",
    "- **Seleção de características**: A principal diferença entre Bagging e Random Forest é a **seleção aleatória de características** no Random Forest, enquanto o Bagging utiliza todas as características disponíveis para dividir os nós das árvores.\n",
    "- **Objetivo da técnica**: Bagging é mais genérico e pode ser aplicado a diferentes tipos de modelos, enquanto o Random Forest é uma aplicação específica de Bagging para **árvores de decisão** com uma modificação que melhora sua performance.\n",
    "\n",
    "Em resumo, o **Random Forest é uma versão mais sofisticada e robusta do Bagging**, com a adição da seleção aleatória das características para garantir maior diversidade e precisão no modelo final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspeto | Bagging | Random Forest |\n",
    "| --- | --- | --- |\n",
    "| Modelo base | Qualquer modelo | Árvores de decisão |\n",
    "| Amostragem | Bootstrap sampling | Bootstrap sampling |\n",
    "| Seleção de funcionalidades | Todas as funcionalidades disponíveis | Subconjunto aleatório de funcionalidades |\n",
    "| Combinação das previsões | Maioria ou média das previsões | Maioria ou média das previsões |\n",
    "| Variância | Reduzida | Reduzida (mais do que o bagging) |\n",
    "| Diversidade entre os modelos | Menor | Maior, graças à seleção aleatória das funcionalidades |\n",
    "| Complexidade | Menor | Maior |\n",
    "\n",
    "Em resumo, enquanto o bagging aplica o bootstrap sampling para criar diferentes conjuntos de dados e treinar modelos independentes, o Random Forest vai além, introduzindo também a aleatoriedade na seleção das funcionalidades em cada nó das árvores de decisão, resultando em maior diversidade e, frequentemente, melhores desempenhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparâmetros da Random Forest:\n",
    "\n",
    "- **n_estimators:** Este é o número de árvores que serão construídas na floresta. Um número maior de árvores pode melhorar o desempenho do modelo, mas também aumenta o tempo de treinamento.\n",
    "- **criterio:** Define a função utilizada para medir a qualidade de uma divisão em cada nó da árvore. \"Gini\" mede a impureza de uma divisão, enquanto \"entropy\" mede o ganho de informação.\n",
    "- **max_depth:** Especifica a profundidade máxima de cada árvore na floresta. Limitar a profundidade das árvores pode ajudar a evitar overfitting.\n",
    "- **min_samples_split:** Define o número mínimo de amostras necessárias em um nó para que uma divisão seja considerada. Isso pode ajudar a controlar o crescimento da árvore, evitando divisões que resultam em nós com poucas amostras.\n",
    "- **min_samples_leaf:** Especifica o número mínimo de amostras necessárias para ser um nó folha. Isso ajuda a evitar folhas com poucas amostras, o que pode resultar em árvores excessivamente especializadas.\n",
    "- **min_weight_fraction_leaf:** Semelhante ao `min_samples_leaf`, mas expresso como uma fração do número total de amostras ponderadas.\n",
    "- **max_features:** Define o número máximo de características a serem consideradas em cada divisão. Isso pode influenciar a diversidade das árvores na floresta.\n",
    "- **max_leaf_nodes:** Especifica o número máximo de nós folha na árvore. Pode ser usado no lugar de `max_depth` para controlar o crescimento das árvores.\n",
    "- **min_impurity_decrease:** Uma divisão será realizada se resultar em uma diminuição da impureza maior ou igual a esse valor. Isso pode ajudar a controlar a complexidade da árvore.\n",
    "- **bootstrap:** Indica se o sampling com reposição deve ser utilizado durante a criação das árvores.\n",
    "- **oob_score:** Indica se os amostras fora da bolsa (out-of-bag) devem ser usadas para estimar o erro de generalização.\n",
    "- **random_state:** Determina a semente aleatória para garantir a reprodutibilidade do modelo.\n",
    "- **verbose:** Controla a quantidade de informações impressas durante o treinamento.\n",
    "- **class_weight:** Define os pesos das classes, útil para lidar com conjuntos de dados desbalanceados.\n",
    "- **ccp_alpha:** Parâmetro de complexidade de custo usado para podar a árvore. Isso pode ajudar a evitar overfitting.\n",
    "\n",
    "Esses hiperparâmetros permitem ajustar a complexidade e o desempenho do modelo Random Forest, tornando-o mais adequado para diferentes tipos de dados e tarefas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
